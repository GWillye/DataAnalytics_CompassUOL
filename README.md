# D&A - AWS [Compass.UOL](https://compass.uol/pt/home/?utm_source=google-ads&utm_medium=ppc&utm_campaign=compasso-uol-institucional&utm_term=compass+uol): Sobre mim

**Ol√°, meu nome √© Gabriel Willye**
**üè°Cidade: [Campo Grande/MS](https://www.google.com.br/maps/place/Campo+Grande,+MS/@-20.6258611,-54.8465322,10z/data=!3m1!4b1!4m6!3m5!1s0x9486f3f8b2834447:0xa35a7db8b968e5fd!8m2!3d-20.6281521!4d-54.5218074!16s%2Fg%2F11rgdh3sd7?entry=ttu)**
**üè´Institui√ß√£o de ensino: [Universidade Federal de Mato Grosso do Sul (UFMS)](https://www.ufms.br/)**
**üë®‚ÄçüéìCurso e semestre atual: Sistemas de Informa√ß√£o - quarto semestre**

## Experi√™ncias na √°rea de tecnologia

Estou no momento em meu primeiro emprego formal na √°rea de Tecnologia, como estagi√°rio da Compass UOL.

Sou animado e extrovertido, fazendo meus projetos e tarefas com muita anima√ß√£o e dedica√ß√£o. Gosto de ensinar e de coordenar tarefas, me dando muito bem em tarefas com gest√£o e colabora√ß√£o entre pessoas. Sou dedicado e amo trabalhos em que posso discorrer, fazer apresenta√ß√µes e demonstrar meus conhecimentos. Acima de tudo, gosto de aprender e ensinar, e acredito que tenho muitas possibilidades para isso ao longo de minha jornada.

## Hobbies

Meus hobbies principais giram em torno de ouvir m√∫sicas (minha playlist no [Spotify](https://open.spotify.com/playlist/60y6pGtZ0K207UEY6QipqZ?si=4da97c4284724c35)), ler livros e estudar conte√∫dos que n√£o s√£o diretamente relacionados com minha carreira, como filosofia, sociologia, psicologia, teologia e hist√≥ria. Estou estudando ingl√™s e pretendo tamb√©m aprender russo.
O restante do meu tempo livre eu normalmente passo envolvido com atividades de minha igreja (sou Adventista do S√©timo Dia), como professor da Escola Sabatina, Sonoplastia e Comunica√ß√£o, e tamb√©m instrutor no Clube de Desbravadores de uma classe com crian√ßas de 13 anos. Al√©m disso, atuo tamb√©m como membro volunt√°rio do Programa de Educa√ß√£o e Tutoria Sistemas em minha universidade e sou o representante discente do curso de Sistemas de Informa√ß√£o.

## Sprint 1 - Markdown, Git e Linux

Ser √°gil faz parte da cultura da Compass, mas afinal, o que √© ser √°gil? Esta primeira se√ß√£o da trilha foi reservada para descobrir, estudar e compreender alguns conceitos que giram em torno disso. Uma das tecnologias mais essenciais para manter as coisas em ordem, e que √© essencial para o sucesso de projetos que envolvem codifica√ß√£o, √© o versionamento de c√≥digo. E, quando se estuda versionamento, muitos dos comandos utilizados s√£o baseados no Sistema Operacional Linux. Assim sendo, nesta primeira sprint, foram estudados sobre o que s√£o e como funcionam Metodologias √Ågeis, o que √© versionamento de c√≥digo e como o utilizar atrav√©s do Git, assim como o sistema operacional Linux, seus principais comandos e configura√ß√µes.

Nesta sprint, conceitos de Metodologia √Ågil, Ger√™ncia de Projetos, SCRUM, Kanban, Spotify Squads (que √© bem semelhante ao modo de trabalho da Compass UOL), Design Sprint Google, S.M.A.R.T., Gest√£o Lean, assim como ferramentas como o Trello e a Asana, Storyboards, Mapas de Empatia, foram aprendidos. Al√©m disso, tamb√©m foram estudados o funcionamento de um Sistema Operacional Linux, os principais comandos do terminal e como configurar um espa√ßo para desenvolvimento. Por √∫ltimo, foi visto como funciona o versionamento de c√≥digo atrav√©s do Git na plataforma do GitHub.

### Sprint 1 - Exerc√≠cios

[Exerc√≠cio 1](Sprint_1/exercicios/ex1.md)  
[Exercicio 2](Sprint_1/exercicios/ex2.md)  
[Exercicio 3](Sprint_1/exercicios/ex3.md)  
[Exercicio 4](Sprint_1/exercicios/ex4.md)

### Sprint 1 - Evid√™ncias

Foram desenvolvidos resumos sobre o estudado em cada curso, e est√£o dispon√≠veis para acesso em Notion.so. No arquivo "Resumos.txt" est√° anexado o link, e os links  dos resumos dos cursos est√£o dispon√≠veis aqui tamb√©m:

* [Resumo do curso de Git](https://gabrielwillye.notion.site/gabrielwillye/Comandos-Git-7f64ad1cb110467bb12d9ffc79396d9a)
* [Resumo do curso de Linux](https://gabrielwillye.notion.site/gabrielwillye/Comandos-Linux-1f45ebb40fdc47b49a16f798aa6bfd04)
* [Resumo do curso de Metodologia √Ågil](https://gabrielwillye.notion.site/gabrielwillye/Metodologias-geis-45da3933fdcd43d79e5915fc6fb57228)

No final do curso de Linux √© apresentado um projeto chamado LAMP, para a instala√ß√£o no Linux do Apache, MySQL e PHP, ensinando tamb√©m a instala√ß√£o do Sublime. O projeto finalizava com dois c√≥digos para exibir os dados das tabelas do MySQL no localhost/index.php.

[conn.php](Sprint_1/evidencias/conn.php)

[index.php](Sprint_1/evidencias/index.php)

Alguns exemplos de comandos utilizados para subir o reposit√≥rio utilizando comandos Git, sendo exibidos pelo comando history:

   [Comandos Git](Sprint_1//evidencias/Git.png)

Port√≥lio criado no projeto do curso de Git:

   [Portfolio do Curso de Git](Sprint_1/evidencias/PrintPortfolio.png)

### Sprint 1 - Certificados

![Certificado do curso de Git](Sprint_1/certificados/Git.jpg)

![Certificado do curso de Linux](Sprint_1/certificados/Linux.jpg)

![Certificado do curso de Metodologia Agil](Sprint_1/certificados/MetodologiaAgil.jpg)

## Sprint 2 - Linguagem SQL e conceitos b√°sicos da √°rea de Dados

Dados s√£o, por muitos, considerados como o recurso mais valioso de nosso s√©culo. A cada segundo, uma quantidade absurda de dados √© produzida, fazendo com que sejam necess√°rios utilizar formas de controlar, acessar e utilizar estes dados de uma maneira pr√°tica e realista. Nesa sprint, foram estudados conceitos sobre Big Data e como este grandioso volume de dados, que s√£o partes do nosso dia-a-dia, funcionam, assim como fazer an√°lises e tratamentos de dados quando consideramos bancos de dados relacionais, que operam com um volume de dados  relativamente menor que o estudado no curso de Big Data.

### Sprint 2 - Exerc√≠cios

Nesta Sprint tivemos dois casos de estudo: Biblioteca e Loja.

#### Caso de Estudo "Biblioteca"

[Exerc√≠cio 1](Sprint_2/exercicios/ex1.sql)

[Exerc√≠cio 2](Sprint_2/exercicios/ex2.sql)

[Exerc√≠cio 3](Sprint_2/exercicios/ex3.sql)

[Exerc√≠cio 4](Sprint_2/exercicios/ex4.sql)

[Exerc√≠cio 5](Sprint_2/exercicios/ex5.sql)

[Exerc√≠cio 6](Sprint_2/exercicios/ex6.sql)

[Exerc√≠cio 7](Sprint_2/exercicios/ex7.sql)

#### Caso de Estudo "Loja"

[Exerc√≠cio 8](Sprint_2/exercicios/ex8.sql)

[Exerc√≠cio 9](Sprint_2/exercicios/ex9.sql)

[Exerc√≠cio 10](Sprint_2/exercicios/ex10.sql)

[Exerc√≠cio 11](Sprint_2/exercicios/ex11.sql)

[Exerc√≠cio 12](Sprint_2/exercicios/ex12.sql)

[Exerc√≠cio 13](Sprint_2/exercicios/ex13.sql)

[Exerc√≠cio 14](Sprint_2/exercicios/ex14.sql)

[Exerc√≠cio 15](Sprint_2/exercicios/ex15.sql)

[Exerc√≠cio 16](Sprint_2/exercicios/ex16.sql)

#### Exporta√ß√£o de Dados

[Exporta√ß√£o de Dados 1](Sprint_2/exercicios/exportacao1.csv)

[Exporta√ß√£o de Dados 2](Sprint_2/exercicios/exportacao2.csv)

### Sprint 2 - Evidencias

Foram desenvolvidos resumos sobre o estudado em cada curso, e est√£o dispon√≠veis para acesso em Notion.so. No arquivo "Resumos.txt" est√° anexado o link, e os links  dos resumos dos cursos est√£o dispon√≠veis aqui tamb√©m:

* [Resumo do curso de SQL para An√°lise de Dados](https://gabrielwillye.notion.site/SQL-para-An-lise-de-Dados-c3292e791c4c422a85b7be54c7c4f1b6?pvs=4)
* [Resumo do curso de Big Data](https://gabrielwillye.notion.site/Fundamentos-de-Big-Data-79a06f2c95214628a5e1a884cb6ce161?pvs=4)

### Sprint 2 - Certificados

![Certificado do curso de SQL para An√°lise de Dados](certificados/../Sprint_2/certificados/SQL.jpg)

![Certificado do curso de Fundamentos em Big Data](certificados/../Sprint_2/certificados/BigData.png)

## Sprint 3 - Python

Uma das linguagens mais populares da atualidade e que possui uma ampla utilidade em projetos de dados, dominar o Python √© essencial para quem segue carreira em Data Science. Nesta sprint, foi estudado de maneira intensiva a respeito de o que √© o Python, como funciona, suas caracter√≠sticas e como utilizar suas ferramentas e recursos na carreira de Data & Analytics.

### Sprint 3 - Exerc√≠cios

Nesta Sprint, tivemos dois blocos de exerc√≠cios em Python:

#### Python I - 1/2

[Exerc√≠cio 1](Sprint_3/exercicios/ex1.py)

[Exerc√≠cio 2](Sprint_3/exercicios/ex2.py)

[Exerc√≠cio 3](Sprint_3/exercicios/ex3.py)

[Exerc√≠cio 4](Sprint_3/exercicios/ex4.py)

[Exerc√≠cio 5](Sprint_3/exercicios/ex5.py)

#### Python 1 - 2/2

[Exerc√≠cio 6](Sprint_3/exercicios/ex6.py)

[Exerc√≠cio 7](Sprint_3/exercicios/ex7.py)

[Exerc√≠cio 8](Sprint_3/exercicios/ex8.py)

[Exerc√≠cio 9](Sprint_3/exercicios/ex9.py)

[Exerc√≠cio 10](Sprint_3/exercicios/ex10.py)

[Exerc√≠cio 11](Sprint_3/exercicios/ex11.py)

[Exerc√≠cio 12](Sprint_3/exercicios/ex12.py)

[Exerc√≠cio 13](Sprint_3/exercicios/ex13.py)

[Exerc√≠cio 14](Sprint_3/exercicios/ex14.py)

[Exerc√≠cio 15](Sprint_3/exercicios/ex15.py)

[Exerc√≠cio 16](Sprint_3/exercicios/ex16.py)

[Exerc√≠cio 17](Sprint_3/exercicios/ex17.py)

[Exerc√≠cio 18](Sprint_3/exercicios/ex18.py)

[Exerc√≠cio 19](Sprint_3/exercicios/ex19.py)

[Exerc√≠cio 20](Sprint_3/exercicios/ex20.py)

#### Python II - 1/2

[Exerc√≠cio 21](Sprint_3/exercicios/ex21.py)

[Exerc√≠cio 22](Sprint_3/exercicios/ex22.py)

[Exerc√≠cio 23](Sprint_3/exercicios/ex23.py)

[Exerc√≠cio 24](Sprint_3/exercicios/ex24.py)

[Exerc√≠cio 25](Sprint_3/exercicios/ex25.py)

#### Python II - 2/2

[Etapa 1 - Desafio - ETL com Python](Sprint_3/exercicios/Desafio/etapa-1.txt)

[Etapa 2 - Desafio - ETL com Python](Sprint_3/exercicios/Desafio/etapa-2.txt)

[Etapa 3 - Desafio - ETL com Python](Sprint_3/exercicios/Desafio/etapa-3.txt)

[Etapa 4 - Desafio - ETL com Python](Sprint_3/exercicios/Desafio/etapa-4.txt)

[Etapa 5 - Desafio - ETL com Python](Sprint_3/exercicios/Desafio/etapa-5.txt)

### Sprint 3 - Evid√™ncias

Foi desenvolvido um caderno de c√≥digos com alguns desafios propostos em aula. Al√©m disso, nele tamb√©m est√° contido o c√≥digo fonte utilizado para completar o Desafio:

* [Caderno de C√≥digos](Sprint_3/evidencias/Notas%20Do%20Curso.ipynb)

### Sprint 3 - Certificados

![Certificado do curso de Python3](certificados/../Sprint_3/certificados/Python3.jpg)

## Sprint 4 - Paradigma Funcional(Python) e Containers

Neste Sprint, aprendemos formas de como utilizar a Programa√ß√£o Funcional, um paradigma muito utilizado, e Docker, uma tecnologia para administrar containers (Containers s√£o projetos que permitem criar um ambiente de desenvolvimento isolado e compartilhar este ambiente com outras pessoas). Al√©m disso, estudamos tecnologias e m√©todos estat√≠sticos, que juntos do Python, formam um conjunto extremamente √∫til e otimizado para a √°rea de Data & Analytics.

### Sprint 4 - Exerc√≠cios

#### Programa√ß√£o Funcional

[Exerc√≠cio 1](Sprint_4/exercicios/ex1.py)

[Exerc√≠cio 2](Sprint_4/exercicios/ex2.py)

[Exerc√≠cio 3](Sprint_4/exercicios/ex3.py)

[Exerc√≠cio 4](Sprint_4/exercicios/ex4.py)

[Exerc√≠cio 5](Sprint_4/exercicios/ex5.py)

#### Exerc√≠cio Docker

[Desafio Docker](Sprint_4/exercicios/DesafioDocker/)

### Sprint 4 - Evid√™ncias

[Link do resumo do curso de Docker](https://gabrielwillye.notion.site/Docker-4beb4a3cf3374fa99f6cd06bcee3531c?pvs=4)

[Link do resumo do curso de Estat√≠stica Descritiva com Python](https://gabrielwillye.notion.site/Estat-stica-Descritiva-com-Python-ee1d1dae1abe4696bb1473b55a75aaad?pvs=4)

[Caderno de Exerc√≠cios - Curso de Estatistica](Sprint_4/evidencias/Estatistica.ipynb)

### Sprint 4 - Certificados

![Certificado do curso de Python3](Sprint_4/certificados/EstDescPy.jpg)
![Certificado do curso de Python3](Sprint_4/certificados/Docker.jpg)

## Sprint 5 - Fundamentos de Computa√ß√£o em Nuvem (AWS)

Nesta Sprint come√ßamos a estudar os fundamentos da Computa√ß√£o em Nuvem com foco em Amazon Web Services e sua tecnologia. Estudamos conte√∫dos que nos auxiliam a compreender o que √© a AWS, como funcionam algumas tecnologias deles e outros conte√∫dos que nos auxiliam a aprender e se preparar para a Certifica√ß√£o Cloud Pratictioner.

### Sprint 5 - Exerc√≠cios

N√£o tivemos exerc√≠cios nesta Sprint

## Sprint 5 - Evid√™ncias

[Resumo do curso de AWS Partner: Sales Accreditation](https://gabrielwillye.notion.site/AWS-Sales-Accreditation-498f1ef430a3482ab7c039a0b80d4f28?pvs=4)

## Sprint 5 - Certificados

![Selo do curso de AWS Partner: Sales Accreditation](Sprint_5/certificados/Sales.png)
![Certificado do curso de AWS Cloud Quest: Cloud Practitioner](Sprint_5/certificados/CloudQuest.png)
![Certificado do curso de AWS Partner: Accreditation](Sprint_5/certificados/Accreditation.png)
![Selo do curso de AWS Partner: Cloud Economics Accreditation](Sprint_5/certificados/SeloCloud.png)
![Certificado do curso Exam Prep: AWS Certified Cloud Practitioner](Sprint_5/certificados/ExamPrep.png)

## Sprint 6 - Servi√ßos Anal√≠ticos (AWS)

Nesta Sprint, foram estudadas as ferramentas e t√©cnicas disponibilizadas pela Amazon Web Services para servi√ßos de Data & Analytics. Al√©m disso, foram executados exerc√≠cios pr√°ticos para aprofundar e treinar os conceitos estudados.

### Sprint 6 - Exerc√≠cios

#### Laborat√≥rio AWS S3

![Print](Sprint_6/exercicios/Lab_AWS_S3.png)

#### Laborat√≥rio AWS Athena

![Print](Sprint_6/exercicios/Lab_AWS_Athena.png)

#### Laborat√≥rio AWS Lambda

![Print](Sprint_6/exercicios/Lab_AWS_Lambda.png)

#### Laborat√≥rio AWS - Limpeza de recursos

### Sprint 6 - Evid√™ncias

[Resumo do curso de AWS Kinesis Analytics](Sprint_6/evidencias/AmazonKinesisAnalytics.md)

[Resumo do curso de AWS Partner: Data Analytics on AWS - Business](Sprint_6/evidencias/DataAnalytics.md)

[Resumo do curso Amazon Kinesis Streams](Sprint_6/evidencias/AmazonKinesisStreams.md)

[Resumo do curso Amazon Elastic MapReduce](Sprint_6/evidencias/AmazonEMR.md)

[Resumo do curso Amazon Athena](Sprint_6/evidencias/AmazonAthena.md)

[Resumo do curso AWS IoT Analytics](Sprint_6/evidencias/IoTAnalytics.md)

[Resumo do curso Getting Started with Amazon Redshift](Sprint_6/evidencias/AmazonRedshift.md)

[Resumo do curso Why Analytics for Games](Sprint_6/evidencias/AnalyticsGames.md)

### Sprint 6 - Certificados

![Certificado de conclus√£o do curso de Amazon Kinesis Analytics](Sprint_6/certificados/KinesisAnalytics.png)
![Certificado de conclus√£o do curso de AWS Partner: Data Analytics on AWS - Business](Sprint_6/certificados/DataAnalytics.png)
![Certificado de conclus√£o do curso de AWS Partner: Data Analytics Fundamentals](Sprint_6/certificados/DataAnalyticsFundamentals.png)
![Certificado de conclus√£o do curso Amazon Kinesis Streams](Sprint_6/certificados/AmazonKinesisStreams.png)
![Certificado de conclus√£o do curso Amazon Elastic MapReduce](Sprint_6/certificados/AmazonEMR.png)
![Certificado de conclus√£o do curso Amazon Athena](Sprint_6/certificados/AmazonAthena.png)
![Certificado de conclus√£o do curso Amazon Quicksight](Sprint_6/certificados/AmazonQuicksight.png)
![Certificado de conclus√£o do curso AWS IoT Analytics](Sprint_6/certificados/IoTAnalytics.png)
![Certificado de conclus√£o do curso Getting Started with Amazon Redshift](Sprint_6/certificados/AmazonRedshift.png)
![Certificado de conclus√£o do curso Deep Dive into Concepts and Tools for Analyzing Streaming Data](Sprint_6/certificados/DeepDive.png)
![Certificado de conclus√£o do curso Best Practices for Data Warehousing with Amazon Redshift](Sprint_6/certificados/DataWarehousing.png)
![Certificado de conclus√£o do curso Serverless Analytics](Sprint_6/certificados/ServerlessAnalytics.png)
![Certificado de conclus√£o do curso Why Analytics for Games](Sprint_6/certificados/AnalyticsGames.png)

## Sprint 7 - Plataformas Big Data (Spark, Hadoop)

Nesta Sprint, foi iniciado o desenvolvimento do Desafio Final, al√©m de se estudar e entender os conceitos por tr√°s do HDSF, ou Hadoop Distributed File System, al√©m de conhecer e entender o funcionamento pr√°tico do Spar, uma das ferramentas mais poderosas no processamento e transforma√ß√£o de dados.

### Sprint 7 - Exerc√≠cios

#### Tarefa 1 - Python com Pandas e Numpy

Os c√≥digos foram executados em .ipynb, em um Notebook no Jupyter Notebooks. Os c√≥digos e respostas podem ser acessados [clicando aqui](Sprint_7/exercicios/Exerc√≠cio-1.ipynb).

#### Tarefa 2 - Apache Spark - Contador de Palavras

O Exerc√≠cio solicita que seja criado um container com a imagem jupyter/spark. Este container ent√£o deve levantar um notebook Jupyter e executar, utilizando docker exec, um algoritmo no pyspark que leia a quantidade de palavras iguais no README.md deste reposit√≥rio.
O comando utilizado para criar o container foi:
**sudo docker run -it -p 8888:8888 --name jupyter_001 jupyter/all-spark-notebook**
Os prints da execu√ß√£o do Jupyter em funcionamento:
![Print 1](Sprint_7/exercicios/Print1_Ex2.png)
![Print 2](Sprint_7/exercicios/Print2_Ex2.png)

O log final pode ser visto no documento est√° dispon√≠vel logo abaixo:

[Log_Exerc√≠cio2.txt](Sprint_7/exercicios/Log_Exerc√≠cio2.txt)

#### Laborat√≥rio - AWS Glue

O print de conclus√£o deste exerc√≠cio est√° dispon√≠vel logo abaixo:

![Print](Sprint_7/exercicios/AWSGlue.png)

[Script do Job](Sprint_7/exercicios/Script_Job.txt)

#### Desafio - Parte I - ETL

O script em Python para subir os arquivos movies.csv e series.csv pode ser acessado [aqui](Desafio/etapa-1/Script.py).

O [Dockerfile](Desafio/etapa-1/Dockerfile) do container.

![Print dos arquivos CSV no S3](Desafio/etapa-1/Desafio.png)

### Sprint 7 - Evid√™ncias

[Resumo do curso de Hadoop](Sprint_7/evidencias/Hadoop.md)

[Resumo do curso Forma√ß√£o Spark com Pyspark](Sprint_7/evidencias/Spark.md)

### Sprint 7 - Certificados

![Learn by Example: Hadoop, Map Reduce for Big Data problems](Sprint_7/certificados/Hadoop.png)
![Forma√ß√£o Spark com Pyspark: O Curso Completo](Sprint_7/certificados/Spark.png)

## Sprint 8 -  Apache Spark

Na Sprint 8, estivemos estudando um pouco mais sobre como funciona a Engenharia de Dados utilizando o Apache Spark. Foi realizado o prosseguimento no Desafio, al√©m de outros tr√™s exerc√≠cios para se familiarizar mais com ferramentas e rotinas de um engenheiro de dados.

### Sprint 8 - Exerc√≠cios

#### Exerc√≠cio 1 - TMDB

O Exerc√≠cio 1 consistia em criar um cadastro na plataforma do TMDB e criar uma chave para utilizar a API de desenvolvimento deles.

Os prints de conclus√£o desta atividade est√£o dispon√≠veis no diret√≥rio [evidencias](Sprint_8/evidencias/).

![Print 1](Sprint_8/evidencias/Ex_1.1.png)
![Print 2](Sprint_8/evidencias/Ex_1.2.png)

#### Exerc√≠cio 2 - Desafio Parte 2: Ingest√£o de dados do TMDB

A parte 2 do Desafio pode ser dividida em 5 partes:

1. Ler os dados de movies.csv e series.csv e salv√°-los em JSON com no m√°ximo 10 MB de tamanho
2. Tratar os dados dos arquivos json gerados
3. Preencher os dados 'NULL' dos arquivos com informa√ß√µes coletadas usando a API do TMDB
4. Salvar os arquivos atualizados em JSON, com 100 registros cada, no S3
5. Transformar esta fun√ß√£o em uma camada do AWS Lambda

A conclus√£o dos primeiros quatro passos est√° dispon√≠vel no seguinte script: [Script do Desafio](/Desafio/etapa-2/Desafio-2.py).
N√£o consegui estar concluindo este √∫ltimo passo.

#### Exerc√≠cio 3 - Gera√ß√£o de massa de dados

O Exerc√≠cio 3 come√ßava com 2 aquecimentos. O primeiro exigia a cria√ß√£o de uma lista com 250 n√∫meros aleat√≥rios entre 1 a 1000. Ap√≥s isso, devia se utilizar a fun√ß√£o *reverse()* e imprimir a lista. O segundo solicitava a cria√ß√£o de uma lista com 20 animais, a ordena√ß√£o da mesma em ordem alfab√©tica e sua escrita em um arquivo ent√£o se devia imprimir o resultado e salvar em um arquivo csv.

[Warm Up 1 - Script](Sprint_8/exercicios/Ex_2_WarmUp_1.py)

![Print do Script ap√≥s execu√ß√£o](Sprint_8/evidencias/Ex_2_WarmUp_1.png)

[Warm Up 2 - Script](Sprint_8/exercicios/Ex_2_WarmUp_2.py)

[Arquivo CSV gerado](Sprint_8/evidencias/animais_ord.csv)

![Print do Script ap√≥s execu√ß√£o](Sprint_8/evidencias/Ex_2_WarmUp_2.png)

O Exerc√≠cio 3, ap√≥s o aquecimento, solicitava a instala√ß√£o da biblioteca *names* , e ent√£o a cria√ß√£o de um script em Python que gerasse de forma pseudoaleat√≥ria uma lista com um milh√£o de nomes aleat√≥rios e 3000 nomes √∫nicos. Ap√≥s a conclus√£o, o resultado devia ser salvo em um arquivo txt com os nomes. O arquivo e o script est√£o dispon√≠veis abaixo.

[Notebook utilizado na execu√ß√£o do exerc√≠cio](Sprint_8/exercicios/Ex_2.ipynb)

[Arquivo txt gerado](Sprint_8/evidencias/nomes_aleatorios.zip)

![Print 1 do Script ap√≥s execu√ß√£o](Sprint_8/evidencias/Ex_2.1.png)
![Print 2 do Script ap√≥s execu√ß√£o](Sprint_8/evidencias/Ex_2.2.png)

#### Exerc√≠cio 4 - Apache Spark

O exerc√≠cio 4 envolvia em desenvolver scripts utilizando o txt nomes aleatorios, gerado no Exerc√≠cio 3. Os scripts criados est√£o abaixo:

[Parte 1](Sprint_8/exercicios/Ex4_1.py)

[Parte 2](Sprint_8/exercicios/Ex4_2.py)

[Parte 3](Sprint_8/exercicios/Ex4_3.py)

[Parte 4](Sprint_8/exercicios/Ex4_4.py)

[Parte 5](Sprint_8/exercicios/Ex4_5.py)

[Parte 6](Sprint_8/exercicios/Ex4_6.py)

[Parte 7](Sprint_8/exercicios/Ex4_7.py)

[Parte 8](Sprint_8/exercicios/Ex4_8.py)

[Parte 9](Sprint_8/exercicios/Ex4_9.py)

[Parte 10](Sprint_8/exercicios/Ex4_10.py)

### Sprint 8 - Evid√™ncias

As evid√™ncias de conclus√£o dos exerc√≠cios foram todas citadas na aba acima.

### Sprint 8 - Certificados

Nesta Sprint, nenhum curso foi realizado.

## Sprint 9 -  Apache Spark

Nesta Sprint, foram realizados exerc√≠cios envolvendo MOdelagem Relacional e Modelagem Dimensional, al√©m de dar continuidade no Desafio, criando as camadas Trusted e Refined.

### Sprint 9 - Exerc√≠cios

#### Tarefa 1 - Modelagem Relacional - Normaliza√ß√£o

Nesta tarefa, era concedido o arquivo [concessionaria.sqlite](Sprint_9/evidencias/concessionaria.sqlite), e o objetivo era transformar esta tabela existente no arquivo em um modelo relacional normalizado. Para isto, foi utilizado o [ScriptConvert.py](Sprint_9/exercicios/ScriptConvert.py), que converteu o arquivo [concessionaria.sqlite](Sprint_9/evidencias/concessionaria.sqlite) para o arquivo [concessionaria.csv](Sprint_9/exercicios/concessionaria.csv), facilitando obter as informa√ß√µes e construir o dump normalizado.
O Dump normalizado pode ser acessado est√° salvo no arquivo [concessionaria.sql](Sprint_9/exercicios/concessionaria.sql).

#### Tarefa 2 - Modelagem Dimensional - Cria√ß√£o de Modelo

Nesta tarefa o banco de dados concessionaria, criado no exerc√≠cio acima, foi readaptado para o modelo dimensional, organizando as suas tabelas em tabelas de dimens√£o e transformando a tabela Loca√ß√£o em uma tabela de fatos. O dump est√° dispon√≠vel no arquivo [consessionaria_dimensionada.sql](Sprint_9/exercicios/concessionaria_dimensionada.sql).

Os exerc√≠cios referentes ao Desafio podem ser encontrados no [ReadMe do Desafio - Parte III](#parte-iii---processamento-da-trusted-e-refined).

### Sprint 9 - Evid√™ncias

[Resumo do curso Exemplo](Exemplo)

### Sprint 9 - Certificados

Nesta Sprint, nenhum certificado foi obtido.

## Sprint 10 - Visualiza√ß√£o de Dados

Nesta Sprint aprendemos sobre como funciona a visualiza√ß√£o de dados para an√°lise, como desenvolver um dashboard e montar uma an√°lise para obter insights. Al√©m disso, por ser a finaliza√ß√£o do programa de bolsas, tamb√©m fiz o curso Seguran√ßa em Aplica√ß√µes Web, entendendo alguns dos princ√≠pios de seguran√ßa que podem ser observados na internet e que precisamos tomar cuidado com o desenvolvimento de quaisquer aplica√ß√µes e scripts.

### Sprint 10 - Exerc√≠cios

N√£o tivemos exerc√≠cios para desenvolver nesta Sprint.

### Sprint 10 - Evid√™ncias

Todas as evid√™ncias do desenvolvimento do Desafio nesta etapa est√£o presentes na pasta [Desafio/etapa-4/](../Desafio/etapa-4/).

### Sprint 10 - Certificados

![Quicksight](certificados/Quicksight.png)
![Seguran√ßa em Aplica√ß√µes Web](certificados/SegApliWeb.png)

## Desafio

O desafio teve in√≠cio na Sprint 7 e est√° sendo desenvolvido at√© o presente momento.

### Parte I - ETL

O script em Python para subir os arquivos movies.csv e series.csv pode ser acessado [aqui](Desafio/etapa-1/Script.py).

O [Dockerfile](Desafio/etapa-1/Dockerfile) do container.

![Print dos arquivos CSV no S3](Desafio/etapa-1/Desafio.png)

### Parte II - Ingest√£o de Dados do TMDB

A parte 2 do Desafio pode ser dividida em 5 partes:

1. Ler os dados de movies.csv e series.csv e salv√°-los em JSON com no m√°ximo 10 MB de tamanho
2. Tratar os dados dos arquivos json gerados
3. Preencher os dados 'NULL' dos arquivos com informa√ß√µes coletadas usando a API do TMDB
4. Salvar os arquivos atualizados em JSON, com 100 registros cada, no S3
5. Transformar esta fun√ß√£o em uma camada do AWS Lambda

A conclus√£o dos primeiros quatro passos est√° dispon√≠vel no seguinte notebook: [Notebook do Desafio](/Desafio/etapa-2/Desafio-2.ipynb).
A conclus√£o do √∫ltimo passo pode ser averiguada [neste print](/Desafio/etapa-2/print_Lambda.png).

### Parte III - Processamento da Trusted e Refined

## Parte III - Processamento da Trusted e Refined

### Parte 1 - Processamento da Trusted
Nesta etapa, seria necess√°rio criar um script que pudesse se conectar a API, ler os arquivos .json gerados e preencher com os dados que faltam. Por√©m, esta etapa foi feita na parte anterior, o que acelerou o desenvolvimento nesta. Na parte 1, foi desenvolvido um script em Python chamado [ScriptJsonToParquet](Desafio/etapa-3/ScriptJsonToParquet.py) que converteu os arquivos com 100 registros .json para o formato .parquet. Ap√≥s isso, foi criado um script em Python chamado [ScriptSubirS3](Desafio/etapa-3/ScriptSubirS3.py) que subiu os arquivos para o S3 na pasta 'compass-uol-desafio/'. Ap√≥s isso, se utilizou um segundo script Python chamado [ScriptMoverParquet](Desafio/etapa-3/ScriptMoverParquet.py) para mover os arquivos .parquet para uma subpasta chamada 'trusted/'.

### Parte 2 - Modelagem de dados da Refined

Nesta etapa, criou-se um modelo dimensional para a Refined, organizando os dados das tabelas movies.csv e series.csv. O modelo dimensional est√° dispon√≠vel no formato SQL no arquivo [modeloDimensional.sql](Desafio/etapa-3/modeloDimensional.sql).
O Script para execu√ß√£o deste modelo e cria√ß√£o do Dataframe da Refined foi executado no AWS Glue, mas sem sucesso at√© o presente momento.

### Parte 3 - Processamento da Refined

Para conseguir criar arquivos correspondentes aos dados desejados como tabelas na Refined, ap√≥s definir o modelo, foram executados 3 jobs no AWS Glue para filtrar os dados da Trusted e os salvar em arquivos .parquet com todos os dados desejados.

- Para criar a tabela DimAtor foi utilizado o Job [DimAtor.py](Desafio/etapa-3/DimAtor.py);
- Para criar a tabela DimObra foi utilizado o Job [DimObra.py](Desafio/etapa-3/DimObra.py);
- Para criar a tabela FatosPersonagem foi utilizado o Job [FatosPersonagem.py](Desafio/etapa-3/FatosPersonagem.py);

### Etapa IV - Apresenta√ß√£o do Dashboard

No dia 02 de novembro de 2023, assisti o v√≠deo [O Fim da Disney](https://www.youtube.com/watch?v=JAg7OQq9vpA) do canal [IMPERA](https://www.youtube.com/@RenatoIMPERA), onde ele comenta sobre a hist√≥ria da Disney, sua ascens√£o e decl√≠nio. O v√≠deo me impressionou muito por sua qualidade t√©cnica e por toda a busca que foi feita, o que acabou me despertando a curiosidade de pesquisar mais e analisar sobre a influ√™ncia da Disney no mercado de anima√ß√µes. As perguntas que eu me propus a responder foram:

1. Ap√≥s o primeiro filme de anima√ß√£o da Disney (Branca de Neve), qual foi a rea√ß√£o do mercado de anima√ß√µes em termos de produ√ß√µes?
2. Como foi a recep√ß√£o p√∫blica dos filmes em anima√ß√£o em compara√ß√£o com os outros g√™neros de filmes?
3. As outras empresas de filmes j√° produziam filmes animados?
4. A Disney influenciou o mercado de filmes de anima√ß√µes? Como?

Ap√≥s definir as perguntas e questionamentos, foi iniciado a coleta de dados necess√°rios para o desenvolvimento do Dashboard. O primeiro passo foi criar uma coluna "produtora", para receber as informa√ß√µes das produtoras de filmes e s√©ries de com√©dia e anima√ß√£o. Ap√≥s isso, tratar estes dados e prepar√°-los para a an√°lise. Para fazer isso, conectei a API do TMDB com o arquivo [movies.csv](https://challenger-uol.s3.amazonaws.com/raw/movies.csv), presente na minha [raw](https://challenger-uol.s3.amazonaws.com/raw/) mandei criar uma nova coluna chamada 'produtora' e inserir os dados das produtoras de todos os filmes e salvei o arquivo em minha [trusted](https://challenger-uol.s3.amazonaws.com/trusted/), junto dos arquivos atualizados dos meus Jsons. Ap√≥s isto, exclu√≠ as informa√ß√µes desnecess√°rias do csv e mandei atualizar meus arquivos Json com os dados dos filmes de anima√ß√£o e criei arquivos em .parquet a partir destes .json. Salvei estes resultados na minha [refined](https://challenger-uol.s3.amazonaws.com/trusted/). 
Para atualizar o csv, foram usados os scripts presentes na pasta [etapa-4](etapa-4/). O arquivo [1FiltrarDados.py](Desafio/etapa-4/1FiltrarDados.py) filtra os dados desejados, que s√£o os filmes de anima√ß√£o, do arquivo movies.csv, o script [2RemoverColuna.py](Desafio/etapa-4/2RemoverColuna.py) remove as colunas que n√£o ser√£o usadas no restante do processo, enquanto o script [3RemoveLinhas.py](Desafio/etapa-4/3RemoveLinhas.py) remove as linhas duplicadas presentes no arquivo. Ap√≥s isto, o script [4insereProdutora](Desafio/etapa-4/4insereProdutora) cria uma nova coluna 'produtora'. Por fim, o script [5insereDadosGerais](Desafio/etapa-4/5insereDadosGerais) insere os dados buscados pela API no nosso arquivo csv. Ap√≥s renomear o arquivo e apagar os arquivos intermedi√°rios, o resultado final √© o arquivo [animation_movies.csv](Desafio/etapa-4/animation_movies.csv), que ap√≥s ser convertido para [animation_movies.xlsx](Desafio/etapa-4/animation_movies.xlsx) foi utilizado no desenvolvimento do [Dashboard](Desafio/etapa-4/Dashboard.pdf) no AWS Quicksight.