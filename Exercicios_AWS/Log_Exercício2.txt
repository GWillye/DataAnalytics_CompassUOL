23/10/13 13:26:26 WARN Utils: Your hostname, gabrielwillye resolves to a loopback address: 127.0.1.1; using 10.0.0.114 instead (on interface wlp0s20f3)
23/10/13 13:26:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/10/13 13:26:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/10/13 13:26:27 INFO SparkContext: Running Spark version 3.5.0
23/10/13 13:26:27 INFO SparkContext: OS info Linux, 6.2.0-34-generic, amd64
23/10/13 13:26:27 INFO SparkContext: Java version 17.0.8.1
23/10/13 13:26:27 INFO ResourceUtils: ==============================================================
23/10/13 13:26:27 INFO ResourceUtils: No custom resources configured for spark.driver.
23/10/13 13:26:27 INFO ResourceUtils: ==============================================================
23/10/13 13:26:27 INFO SparkContext: Submitted application: JavaWordCount
23/10/13 13:26:27 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/10/13 13:26:27 INFO ResourceProfile: Limiting resource is cpu
23/10/13 13:26:27 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/10/13 13:26:27 INFO SecurityManager: Changing view acls to: gwillye
23/10/13 13:26:27 INFO SecurityManager: Changing modify acls to: gwillye
23/10/13 13:26:27 INFO SecurityManager: Changing view acls groups to: 
23/10/13 13:26:27 INFO SecurityManager: Changing modify acls groups to: 
23/10/13 13:26:27 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: gwillye; groups with view permissions: EMPTY; users with modify permissions: gwillye; groups with modify permissions: EMPTY
23/10/13 13:26:27 INFO Utils: Successfully started service 'sparkDriver' on port 35251.
23/10/13 13:26:27 INFO SparkEnv: Registering MapOutputTracker
23/10/13 13:26:27 INFO SparkEnv: Registering BlockManagerMaster
23/10/13 13:26:27 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/10/13 13:26:27 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/10/13 13:26:27 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/10/13 13:26:27 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-40121381-222c-44f9-87a7-835a6e9c7750
23/10/13 13:26:27 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
23/10/13 13:26:27 INFO SparkEnv: Registering OutputCommitCoordinator
23/10/13 13:26:28 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
23/10/13 13:26:28 INFO Utils: Successfully started service 'SparkUI' on port 4040.
23/10/13 13:26:28 INFO SparkContext: Added JAR file:///opt/spark/examples/jars/spark-examples_2.12-3.5.0.jar at spark://10.0.0.114:35251/jars/spark-examples_2.12-3.5.0.jar with timestamp 1697217987293
23/10/13 13:26:28 INFO SparkContext: Added JAR file:///opt/spark/examples/jars/scopt_2.12-3.7.1.jar at spark://10.0.0.114:35251/jars/scopt_2.12-3.7.1.jar with timestamp 1697217987293
23/10/13 13:26:28 WARN SparkContext: The JAR file:/opt/spark/examples/jars/spark-examples_2.12-3.5.0.jar at spark://10.0.0.114:35251/jars/spark-examples_2.12-3.5.0.jar has been added already. Overwriting of added jar is not supported in the current version.
23/10/13 13:26:28 INFO Executor: Starting executor ID driver on host 10.0.0.114
23/10/13 13:26:28 INFO Executor: OS info Linux, 6.2.0-34-generic, amd64
23/10/13 13:26:28 INFO Executor: Java version 17.0.8.1
23/10/13 13:26:28 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/10/13 13:26:28 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@3330f3ad for default.
23/10/13 13:26:28 INFO Executor: Fetching spark://10.0.0.114:35251/jars/scopt_2.12-3.7.1.jar with timestamp 1697217987293
23/10/13 13:26:28 INFO TransportClientFactory: Successfully created connection to /10.0.0.114:35251 after 38 ms (0 ms spent in bootstraps)
23/10/13 13:26:28 INFO Utils: Fetching spark://10.0.0.114:35251/jars/scopt_2.12-3.7.1.jar to /tmp/spark-7ede28fd-5598-4b4e-b636-a71b8c09bbe0/userFiles-61806932-7128-469a-8ffd-04e7f47685cc/fetchFileTemp15221874699617969153.tmp
23/10/13 13:26:28 INFO Executor: Adding file:/tmp/spark-7ede28fd-5598-4b4e-b636-a71b8c09bbe0/userFiles-61806932-7128-469a-8ffd-04e7f47685cc/scopt_2.12-3.7.1.jar to class loader default
23/10/13 13:26:28 INFO Executor: Fetching spark://10.0.0.114:35251/jars/spark-examples_2.12-3.5.0.jar with timestamp 1697217987293
23/10/13 13:26:28 INFO Utils: Fetching spark://10.0.0.114:35251/jars/spark-examples_2.12-3.5.0.jar to /tmp/spark-7ede28fd-5598-4b4e-b636-a71b8c09bbe0/userFiles-61806932-7128-469a-8ffd-04e7f47685cc/fetchFileTemp1577154396730017610.tmp
23/10/13 13:26:28 INFO Executor: Adding file:/tmp/spark-7ede28fd-5598-4b4e-b636-a71b8c09bbe0/userFiles-61806932-7128-469a-8ffd-04e7f47685cc/spark-examples_2.12-3.5.0.jar to class loader default
23/10/13 13:26:28 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45479.
23/10/13 13:26:28 INFO NettyBlockTransferService: Server created on 10.0.0.114:45479
23/10/13 13:26:28 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/10/13 13:26:28 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.0.114, 45479, None)
23/10/13 13:26:28 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.0.114:45479 with 434.4 MiB RAM, BlockManagerId(driver, 10.0.0.114, 45479, None)
23/10/13 13:26:28 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.0.114, 45479, None)
23/10/13 13:26:28 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.0.114, 45479, None)
23/10/13 13:26:29 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/10/13 13:26:29 INFO SharedState: Warehouse path is 'file:/home/gwillye/Documentos/DataAnalytics_CompassUOL/spark-warehouse'.
23/10/13 13:26:30 INFO InMemoryFileIndex: It took 56 ms to list leaf files for 1 paths.
23/10/13 13:26:33 INFO FileSourceStrategy: Pushed Filters: 
23/10/13 13:26:33 INFO FileSourceStrategy: Post-Scan Filters: 
23/10/13 13:26:33 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.6 KiB, free 434.2 MiB)
23/10/13 13:26:33 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
23/10/13 13:26:33 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.0.114:45479 (size: 34.5 KiB, free: 434.4 MiB)
23/10/13 13:26:33 INFO SparkContext: Created broadcast 0 from javaRDD at JavaWordCount.java:45
23/10/13 13:26:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/10/13 13:26:34 INFO SparkContext: Starting job: collect at JavaWordCount.java:53
23/10/13 13:26:34 INFO DAGScheduler: Registering RDD 6 (mapToPair at JavaWordCount.java:49) as input to shuffle 0
23/10/13 13:26:34 INFO DAGScheduler: Got job 0 (collect at JavaWordCount.java:53) with 1 output partitions
23/10/13 13:26:34 INFO DAGScheduler: Final stage: ResultStage 1 (collect at JavaWordCount.java:53)
23/10/13 13:26:34 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
23/10/13 13:26:34 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
23/10/13 13:26:34 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[6] at mapToPair at JavaWordCount.java:49), which has no missing parents
23/10/13 13:26:34 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 25.6 KiB, free 434.1 MiB)
23/10/13 13:26:34 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 11.7 KiB, free 434.1 MiB)
23/10/13 13:26:34 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.0.114:45479 (size: 11.7 KiB, free: 434.4 MiB)
23/10/13 13:26:34 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580
23/10/13 13:26:34 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[6] at mapToPair at JavaWordCount.java:49) (first 15 tasks are for partitions Vector(0))
23/10/13 13:26:34 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/10/13 13:26:34 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.0.0.114, executor driver, partition 0, PROCESS_LOCAL, 8512 bytes) 
23/10/13 13:26:34 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/10/13 13:26:35 INFO CodeGenerator: Code generated in 291.812323 ms
23/10/13 13:26:35 INFO FileScanRDD: Reading File path: file:///home/gwillye/Documentos/DataAnalytics_CompassUOL/README.md, range: 0-16299, partition values: [empty row]
23/10/13 13:26:35 INFO CodeGenerator: Code generated in 20.334907 ms
23/10/13 13:26:35 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1846 bytes result sent to driver
23/10/13 13:26:35 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 845 ms on 10.0.0.114 (executor driver) (1/1)
23/10/13 13:26:35 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/10/13 13:26:35 INFO DAGScheduler: ShuffleMapStage 0 (mapToPair at JavaWordCount.java:49) finished in 1,132 s
23/10/13 13:26:35 INFO DAGScheduler: looking for newly runnable stages
23/10/13 13:26:35 INFO DAGScheduler: running: Set()
23/10/13 13:26:35 INFO DAGScheduler: waiting: Set(ResultStage 1)
23/10/13 13:26:35 INFO DAGScheduler: failed: Set()
23/10/13 13:26:35 INFO DAGScheduler: Submitting ResultStage 1 (ShuffledRDD[7] at reduceByKey at JavaWordCount.java:51), which has no missing parents
23/10/13 13:26:35 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 5.2 KiB, free 434.1 MiB)
23/10/13 13:26:35 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.9 KiB, free 434.1 MiB)
23/10/13 13:26:35 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.0.114:45479 (size: 2.9 KiB, free: 434.4 MiB)
23/10/13 13:26:35 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1580
23/10/13 13:26:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (ShuffledRDD[7] at reduceByKey at JavaWordCount.java:51) (first 15 tasks are for partitions Vector(0))
23/10/13 13:26:35 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
23/10/13 13:26:35 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.0.0.114, executor driver, partition 0, NODE_LOCAL, 7712 bytes) 
23/10/13 13:26:35 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
23/10/13 13:26:35 INFO ShuffleBlockFetcherIterator: Getting 1 (9.2 KiB) non-empty blocks including 1 (9.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/10/13 13:26:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 14 ms
23/10/13 13:26:35 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 22097 bytes result sent to driver
23/10/13 13:26:35 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 158 ms on 10.0.0.114 (executor driver) (1/1)
23/10/13 13:26:35 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/10/13 13:26:35 INFO DAGScheduler: ResultStage 1 (collect at JavaWordCount.java:53) finished in 0,187 s
23/10/13 13:26:35 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/10/13 13:26:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
23/10/13 13:26:35 INFO DAGScheduler: Job 0 finished: collect at JavaWordCount.java:53, took 1,433226 s
23/10/13 13:26:35 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.0.114:45479 in memory (size: 11.7 KiB, free: 434.4 MiB)
Analíticos: 1
Redshift](Sprint_6/evidencias/AmazonRedshift.md): 1
2](Sprint_3/exercicios/ex2.py): 1
muita: 1
Exemplo: 1
que: 20
caderno: 1
17](Sprint_3/exercicios/ex17.py): 1
dando: 1
Dive: 1
tenho: 1
isolado: 1
da: 9
Tecnologia,: 1
localhost/index.php.: 1
afinal,: 1
este: 2
Assim: 1
Analytics: 5
Python](Sprint_3/exercicios/Desafio/etapa-5.txt): 1
filosofia,: 1
Reduce: 1
Games](Sprint_6/evidencias/AnalyticsGames.md): 1
Comunicação,: 1
Certified: 1
Tutoria: 1
Containers: 1
criado: 1
tecnologia: 2
Hadoop): 1
Serverless: 1
Federal: 1
pessoas.: 1
25](Sprint_3/exercicios/ex25.py): 1
13: 1
2](Sprint_1/exercicios/ex2.md): 1
Python3](certificados/../Sprint_3/certificados/Python3.jpg): 1
das: 3
exercícios: 3
4](Sprint_4/exercicios/ex4.py): 1
passo: 1
![Certificado: 24
algumas: 1
PHP,: 1
Data](https://gabrielwillye.notion.site/Fundamentos-de-Big-Data-79a06f2c95214628a5e1a884cb6ce161?pvs=4): 1
###: 19
me: 1
meus: 2
exibidos: 1
longo: 1
trabalho: 1
Grosso: 1
crianças: 1
Map: 1
teologia: 1
em: 19
Gerência: 1
estatísticos,: 1
nome: 1
[index.php](Sprint_1/evidencias/index.php): 1
Numpy: 1
modo: 1
psicologia,: 1
Python](Sprint_3/exercicios/Desafio/etapa-3.txt): 1
conclusão: 13
Sabatina,: 1
4](Sprint_3/exercicios/ex4.py): 1
por: 1
Evidencias: 1
colaboração: 1
recursos: 2
estão: 4
aprendidos.: 1
1](Sprint_4/exercicios/ex1.py): 1
Foi: 1
sou: 1
Business](Sprint_6/certificados/DataAnalytics.png): 1
fazendo: 2
pessoas).: 1
![Print](Sprint_6/exercicios/Lab_AWS_S3.png): 1
final: 1
Documentos: 4
tabelas: 1
Nesa: 1
1](Sprint_1/exercicios/ex1.md): 1
Streaming: 1
2](Sprint_4/exercicios/ex2.py): 1
Docker: 1
ao: 2
com: 20
Parte: 1
(que: 1
Docker,: 1
Sprint: 35
uma: 5
ensinando: 1
os: 6
[Desafio: 1
23](Sprint_3/exercicios/ex23.py): 1
Pyspark:: 1
útil: 1
sociologia,: 1
O: 3
quarto: 1
Dados: 6
Laboratório: 5
desenvolvimento.: 1
ensinar,: 1
[Compass.UOL](https://compass.uol/pt/home/?utm_source=google-ads&utm_medium=ppc&utm_campaign=compasso-uol-institucional&utm_term=compass+uol):: 1
diretamente: 1
Python](Sprint_3/exercicios/Desafio/etapa-4.txt): 1
12](Sprint_3/exercicios/ex12.py): 1
Gabriel: 1
21](Sprint_3/exercicios/ex21.py): 1
11](Sprint_3/exercicios/ex11.py): 1
ferramentas: 3
8](Sprint_3/exercicios/ex8.py): 1
Elastic: 2
realista.: 1
jornada.: 1
tempo: 1
básicos: 1
Games](Sprint_6/certificados/AnalyticsGames.png): 1
Git](Sprint_1/certificados/Git.jpg): 1
Hadoop](evidencias/Hadoop.md): 1
##: 14
como: 18
Estatistica](Sprint_4/evidencias/Estatistica.ipynb): 1
15](Sprint_2/exercicios/ex15.sql): 1
quando: 2
10: 1
Agil](Sprint_1/certificados/MetodologiaAgil.jpg): 1
Não: 1
instrutor: 1
muitas: 1
Analytics](Sprint_6/certificados/ServerlessAnalytics.png): 1
Ser: 1
18](Sprint_3/exercicios/ex18.py): 1
Amazon: 13
quem: 1
seção: 1
gosto: 1
Big: 6
extrovertido,: 1
coordenar: 1
características: 1
Limpeza: 1
treinar: 1
sobre: 4
I: 2
Ágeis,: 1
Pandas: 1
formam: 1
Programação: 2
16](Sprint_2/exercicios/ex16.sql): 1
22](Sprint_3/exercicios/ex22.py): 1
Visualização: 1
gestão: 1
primeira: 2
10](Sprint_3/exercicios/ex10.py): 1
nosso: 2
Estatística: 1
ouvir: 1
Sobre: 1
Accreditation](certificados/Accreditation.png): 1
Funcional(Python): 1
Sul: 1
2](Sprint_2/exercicios/exportacao2.csv): 1
se: 2
intensiva: 1
24](Sprint_3/exercicios/ex24.py): 1
casos: 1
Gestão: 1
outras: 1
projetos: 4
fazer: 2
3](Sprint_4/exercicios/ex3.py): 1
isso: 1
entre: 1
dedicado: 1
aprendemos: 1
Squads: 1
AWS: 20
Python: 7
técnicas: 1
versionamento: 3
Python3](Sprint_4/certificados/EstDescPy.jpg): 1
coisas: 1
estagiário: 1
funcionam: 2
desenvolvimento: 1
deles: 1
de: 105
Sistemas: 3
1](Sprint_2/exercicios/exportacao1.csv): 1
(Containers: 1
código.: 1
Tools: 1
através: 2
Pyspark](evidencias/Spark.md): 1
Linux.: 1
**👨‍🎓Curso: 1
preparar: 1
o: 24
No: 3
Fundamentos: 2
[conn.php](Sprint_1/evidencias/conn.php): 1
Athena](Sprint_6/evidencias/AmazonAthena.md): 1
recurso: 1
![Learn: 1
Foram: 2
dedicação.: 1
for: 5
Storyboards,: 1
Formação: 1
não: 1
####: 17
plataforma: 1
acessar: 1
Adventista: 1
análises: 1
AWS,: 1
primeiro: 1
desenvolvidos: 2
Mapas: 1
manter: 1
4](Sprint_1/exercicios/ex4.md): 1
breve...**: 3
SCRUM,: 1
foram: 6
fonte: 1
Análise: 2
momento: 1
Fundamentals](Sprint_6/certificados/DataAnalyticsFundamentals.png): 1
**🏡Cidade:: 1
populares: 1
ETL: 6
Serviços: 1
5: 5
Economics: 1
nele: 1
Portólio: 1
do: 63
discorrer,: 1
Web: 2
no: 8
dos: 5
carreira: 2
into: 1
(sou: 1
[Universidade: 1
contido: 1
Design: 1
Athena: 1
aprender: 3
suas: 2
quantidade: 1
partes: 1
"Resumos.txt": 2
Por: 1
7](Sprint_2/exercicios/ex7.sql): 1
Biblioteca: 1
Analytics](Sprint_6/certificados/KinesisAnalytics.png): 1
Completo](certificados/Spark.png): 1
dominar: 1
Python](https://gabrielwillye.notion.site/Estat-stica-Descritiva-com-Python-ee1d1dae1abe4696bb1473b55a75aaad?pvs=4): 1
voluntário: 1
ampla: 1
classe: 1
(minha: 1
possibilidades: 1
15](Sprint_3/exercicios/ex15.py): 1
são,: 1
utilizado,: 1
Computação: 2
2](Sprint_2/exercicios/ex2.sql): 1
também: 6
4: 5
Python:: 1
Esta: 1
subir: 1
sendo,: 1
foi: 3
consideramos: 1
playlist: 1
20](Sprint_3/exercicios/ex20.py): 1
trabalhos: 1
6: 4
são: 5
links: 2
relacionados: 1
Funcional: 1
paradigma: 1
containers: 1
atuo: 1
aula.: 1
sejam: 1
3: 5
cada: 3
16](Sprint_3/exercicios/ex16.py): 1
ensino:: 1
Linux](https://gabrielwillye.notion.site/gabrielwillye/Comandos-Linux-1f45ebb40fdc47b49a16f798aa6bfd04): 1
codificação,: 1
alguns: 2
Practitioner](certificados/CloudQuest.png): 1
Data](Sprint_6/certificados/DeepDive.png): 1
arquivo: 2
disso.: 1
Exportação: 1
formas: 2
Estou: 2
Linux: 3
segue: 1
ser: 1
(UFMS)](https://www.ufms.br/)**: 1
muitos: 1
discente: 1
tudo,: 1
cultura: 1
para: 19
Palavras: 1
operacional: 1
ágil: 1
conhecimentos.: 1
formal: 1
controlar,: 1
Analytics.: 3
auxiliam: 2
versionamento,: 1
SQL: 3
respeito: 1
membro: 1
meu: 3
Metodologias: 1
: 176
8](Sprint_2/exercicios/ex8.sql): 1
dia-a-dia,: 1
finalizava: 1
*: 6
segundo,: 1
Git](https://gabrielwillye.notion.site/gabrielwillye/Comandos-Git-7f64ad1cb110467bb12d9ffc79396d9a): 1
emprego: 1
começamos: 1
[Campo: 1
tarefas,: 1
ágil?: 1
história.: 1
outros: 1
extremamente: 1
estudadas: 1
amo: 1
propostos: 1
apresentado: 1
último,: 1
[Spotify](https://open.spotify.com/playlist/60y6pGtZ0K207UEY6QipqZ?si=4da97c4284724c35)),: 1
projeto: 3
12](Sprint_2/exercicios/ex12.sql): 1
dois: 3
Informação.: 1
[Exportação: 2
disso,: 5
curso: 48
Practices: 1
problems](certificados/Hadoop.png): 1
5](Sprint_3/exercicios/ex5.py): 1
curso,: 2
Clube: 1
acesso: 2
cursos: 2
![Formação: 1
10](Sprint_2/exercicios/ex10.sql): 1
também:: 2
**Em: 3
14](Sprint_2/exercicios/ex14.sql): 1
1/2: 2
**🏫Instituição: 1
possui: 1
E,: 1
Nesta: 6
Lean,: 1
ambiente: 2
espaço: 1
Pratictioner.: 1
Compass,: 1
livre: 1
&: 3
MySQL: 2
Data: 11
livros: 1
Certificação: 1
envolvido: 1
(AWS): 2
Operacional: 2
considerados: 1
principais: 3
tratamentos: 1
relativamente: 1
Partner:: 7
universidade: 1
bem: 2
UOL.: 1
atual:: 1
russo.: 1
repositório: 1
sistema: 1
![Selo: 2
Lambda: 1
Alguns: 1
Apache: 3
Sou: 2
Cloud: 5
as: 2
códigos: 2
normalmente: 1
atualidade: 1
Data](certificados/../Sprint_2/certificados/BigData.png): 1
Redshift](Sprint_6/certificados/DataWarehousing.png): 1
Nuvem: 2
Gosto: 1
Markdown,: 1
Funcional,: 1
Data.: 1
essencial: 2
Além: 5
ordem,: 1
aqui: 2
mais: 3
Analyzing: 1
Services: 2
estudando: 1
Desafio:: 1
**Olá,: 1
Git: 2
prática: 1
Exercício: 1
utilizando: 1
Business](Sprint_6/evidencias/DataAnalytics.md): 1
executados: 1
3](Sprint_1/exercicios/ex3.md): 1
Projetos,: 1
sucesso: 1
Quest:: 1
Sétimo: 1
animação: 1
Dados](https://gabrielwillye.notion.site/SQL-para-An-lise-de-Dados-c3292e791c4c422a85b7be54c7c4f1b6?pvs=4): 1
Accreditation](https://gabrielwillye.notion.site/AWS-Sales-Accreditation-498f1ef430a3482ab7c039a0b80d4f28?pvs=4): 1
conjunto: 1
muito: 2
on: 2
UOL),: 1
Paradigma: 1
[Link: 2
D&A: 1
apresentações: 1
fundamentos: 1
Git:: 1
menor: 1
minha: 4
Neste: 1
Git](Sprint_1/evidencias/PrintPortfolio.png): 1
9: 1
pelo: 1
estudado: 4
permitem: 1
Dados](certificados/../Sprint_2/certificados/SQL.jpg): 1
history:: 1
linguagens: 1
criar: 1
representante: 1
um: 8
Analytics](Sprint_6/evidencias/IoTAnalytics.md): 1
funcionamento: 1
1: 7
volume: 2
Contador: 1
mas: 1
utilizado: 1
grandioso: 1
Concepts: 1
8: 1
desafios: 1
essenciais: 1
1](Sprint_3/exercicios/ex1.py): 1
disponibilizadas: 1
Started: 2
Desbravadores: 1
desenvolvido: 1
otimizado: 1
Hadoop,: 1
Estudo: 2
funciona,: 1
produzida,: 1
juntos: 1
(Spark,: 1
Certificados: 7
muitos,: 1
Desafio: 6
Streams](Sprint_6/certificados/AmazonKinesisStreams.png): 1
nos: 2
Apache,: 1
reservada: 1
Informação: 1
tecnologias: 3
estudo:: 1
Códigos](Sprint_3/evidencias/Notas%20Do%20Curso.ipynb): 1
Prep:: 1
Getting: 2
II: 2
seus: 1
conteúdos: 3
Ágil](https://gabrielwillye.notion.site/gabrielwillye/Metodologias-geis-45da3933fdcd43d79e5915fc6fb57228): 1
conceitos: 5
sprint,: 4
Analytics](Sprint_6/certificados/IoTAnalytics.png): 1
dados: 6
valioso: 1
ler: 1
hobbies: 1
comandos: 5
Hobbies: 1
semelhante: 1
14](Sprint_3/exercicios/ex14.py): 1
Evidências: 6
foco: 1
e: 50
Spark: 5
sendo: 1
5](Sprint_2/exercicios/ex5.sql): 1
Asana,: 1
[Exercicio: 3
Python3](Sprint_4/certificados/Docker.jpg): 1
carreira,: 1
Educação: 1
atividades: 1
Streams](Sprint_6/evidencias/AmazonKinesisStreams.md): 1
necessários: 1
aprofundar: 1
![Print](Sprint_6/exercicios/Lab_AWS_Athena.png): 1
Escola: 1
Mato: 1
Kinesis: 4
A: 1
2/2: 2
#: 1
pela: 1
área: 4
funciona: 1
Meus: 1
terminal: 1
blocos: 1
Plataformas: 1
estudados.: 1
práticos: 1
Linux](Sprint_1/certificados/Linux.jpg): 1
Dia),: 1
descobrir,: 1
GitHub.: 1
9](Sprint_2/exercicios/ex9.sql): 1
Programa: 1
métodos: 1
MapReduce](Sprint_6/certificados/AmazonEMR.png): 1
giram: 2
Docker](https://gabrielwillye.notion.site/Docker-4beb4a3cf3374fa99f6cd06bcee3531c?pvs=4): 1
S3: 1
utilizados: 2
inglês: 1
na: 4
Linux,: 2
código: 3
Google,: 1
exibir: 1
estudar: 3
[Caderno: 2
relacionais,: 1
Caso: 2
S.M.A.R.T.,: 1
exemplos: 1
eu: 1
utilidade: 1
semestre: 1
visto: 1
serviços: 1
Git](Sprint_1//evidencias/Git.png): 1
resumo: 2
trilha: 1
anexado: 2
6](Sprint_2/exercicios/ex6.sql): 1
Spotify: 1
9](Sprint_3/exercicios/ex9.py): 1
link,: 2
pretendo: 1
Deep: 1
5](Sprint_4/exercicios/ex5.py): 1
[Etapa: 5
by: 1
Trello: 1
igreja: 1
Uma: 2
Analytics](Sprint_6/evidencias/AmazonKinesisAnalytics.md): 1
administrar: 1
músicas: 1
3](Sprint_2/exercicios/ex3.sql): 1
[Portfolio: 1
envolvem: 1
Tarefa: 2
Sales: 2
tivemos: 3
Docker](Sprint_4/exercicios/DesafioDocker/): 1
13](Sprint_2/exercicios/ex13.sql): 1
comando: 1
Warehousing: 1
IoT: 2
Estudamos: 1
Science.: 1
acredito: 1
Experiências: 1
Best: 1
nesta: 2
Athena](Sprint_6/certificados/AmazonAthena.png): 1
Practitioner](certificados/ExamPrep.png): 1
instalação: 2
Example:: 1
[Resumo: 16
compartilhar: 1
tecnologia.: 1
faz: 1
Accreditation](certificados/Sales.png): 1
LAMP,: 1
Python](Sprint_3/exercicios/Desafio/etapa-1.txt): 1
7: 4
semestre**: 1
1](Sprint_2/exercicios/ex1.sql): 1
Grande/MS](https://www.google.com.br/maps/place/Campo+Grande,+MS/@-20.6258611,-54.8465322,10z/data=!3m1!4b1!4m6!3m5!1s0x9486f3f8b2834447:0xa35a7db8b968e5fd!8m2!3d-20.6281521!4d-54.5218074!16s%2Fg%2F11rgdh3sd7?entry=ttu)**: 1
6](Sprint_3/exercicios/ex6.py): 1
restante: 1
with: 3
Sonoplastia: 1
2: 6
estudamos: 1
Curso: 3
século.: 1
Redshift](Sprint_6/certificados/AmazonRedshift.png): 1
11](Sprint_2/exercicios/ex11.sql): 1
utilizar: 5
Glue: 1
Empatia,: 1
Willye**: 1
demonstrar: 1
-: 57
torno: 2
"Biblioteca": 1
13](Sprint_3/exercicios/ex13.py): 1
Kanban,: 1
configurações.: 1
funcionam,: 1
Accreditation](certificados/SeloCloud.png): 1
Sublime.: 1
Python,: 2
dados,: 2
posso: 1
19](Sprint_3/exercicios/ex19.py): 1
Exercícios: 8
estuda: 1
"Loja": 1
disponíveis: 4
Loja.: 1
absurda: 1
assim: 3
Sprint,: 3
anos.: 1
Quicksight](Sprint_6/certificados/AmazonQuicksight.png): 1
configurar: 1
3](Sprint_3/exercicios/ex3.py): 1
mim: 1
tarefas: 2
a: 11
Notion.so.: 2
professor: 1
Sistema: 2
Ágil,: 1
estudados: 3
maneira: 2
baseados: 1
Metodologia: 3
Exam: 1
Descritiva: 1
7](Sprint_3/exercicios/ex7.py): 1
Git,: 2
bancos: 1
Why: 2
Python](Sprint_3/exercicios/Desafio/etapa-2.txt): 1
é: 11
![Print](Sprint_6/exercicios/Lab_AWS_Lambda.png): 1
Acima: 1
Compass: 2
estes: 1
[Comandos: 1
parte: 1
[Exercício: 47
ensinar: 1
sua: 1
animado: 1
está: 3
and: 1
chamado: 1
completar: 1
resumos: 4
Linguagem: 1
operam: 1
MapReduce](Sprint_6/evidencias/AmazonEMR.md): 1
4](Sprint_2/exercicios/ex4.sql): 1
compreender: 2
23/10/13 13:26:35 INFO SparkContext: SparkContext is stopping with exitCode 0.
23/10/13 13:26:35 INFO SparkUI: Stopped Spark web UI at http://10.0.0.114:4040
23/10/13 13:26:35 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/10/13 13:26:35 INFO MemoryStore: MemoryStore cleared
23/10/13 13:26:35 INFO BlockManager: BlockManager stopped
23/10/13 13:26:35 INFO BlockManagerMaster: BlockManagerMaster stopped
23/10/13 13:26:35 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/10/13 13:26:35 INFO SparkContext: Successfully stopped SparkContext
23/10/13 13:26:35 INFO ShutdownHookManager: Shutdown hook called
23/10/13 13:26:35 INFO ShutdownHookManager: Deleting directory /tmp/spark-02638607-9047-46d1-ab59-06bead8cd09f
23/10/13 13:26:35 INFO ShutdownHookManager: Deleting directory /tmp/spark-7ede28fd-5598-4b4e-b636-a71b8c09bbe0
